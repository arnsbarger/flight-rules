---
title: "R Trainings: SDAL DSPG 2017"
author: "Madison Arnsbarger"
date: "6/1/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Data - 5/24
```{r Load Data, eval=FALSE}
df <- read.csv("~/sdal/projects/limbo/ArlingtonSchoolProfiles.csv", stringsAsFactors = TRUE)
head(df)

library(rio)
df <- import("~/sdal/projects/limbo/ArlingtonSchoolProfiles.csv")

# LOAD DATA FROM URL/GITHUB
library(RCurl)
url <- getURL("https://jaredlander.com/data/Fake%20Pizza%20Data.csv")
data <- read.csv(textConnection(url))

# LOAD DATA FROM JSON
library(jsonlite)

citibike <- fromJSON("http://citibikenyc.com/stations/json")
  class(citibike) # list
  class(citibike$executionTime) # character
  class(citibike$stationBeanList) # data.frame

stations <- citibike$stationBeanList
colnames(stations)

# FCC LOCATION API EXAMPLE
library(jsonlite)
library(data.table)

FCClocation2FIPS <- function(place_id="VTRC", lon=-77.11577, lat=38.880807) {
  res <- fromJSON(paste0("http://data.fcc.gov/api/block/find?format=json&latitude=", lat, "&longitude=", lon, "&showall=true&format=JSON"))
  data.table(place_id = place_id, 
             state_name = res$State$name, 
             state_fips = res$State$FIPS, 
             county_name = res$County$name, 
             county_fips = res$County$FIPS, 
             block_fips = res$Block$FIPS)
}

FCClocations2FIPS <- function(place_idCol = c("VTRC", "VT-NVC"), lonCol = c(-77.11577, -77.1894815), latCol = c(38.880807, 38.8968325)) {
  res <- as.data.table(t(mapply(FCClocation2FIPS, place_idCol, lonCol, latCol)))
  data.table(place_id = unlist(res$place_id), 
             state_name = unlist(res$state_name), 
             state_fips = unlist(res$state_fips), 
             county_name = unlist(res$county_name), 
             county_fips = unlist(res$county_fips), 
             block_fips = unlist(res$block_fips))
}

# GOOGLE RADAR SEARCH EXAMPLE
radar_search <- fromJSON(paste0("https://maps.googleapis.com/maps/api/place/radarsearch/json?location=", 37.231264, ",", -80.426745, "&radius=1609&type=grocery_or_supermarket&key=AIzaSyC9FKW-kjQlEXjfM3OgMZBJ7xE6zCN1JQI"))

place_id <- "ChIJEauYKUKVTYgR03YCLT7SLeY"

url <- paste0("https://maps.googleapis.com/maps/api/place/details/json?placeid=",
              place_id,
              "&key=AIzaSyC9FKW-kjQlEXjfM3OgMZBJ7xE6zCN1JQI")

detail_search <- fromJSON(url)
head(detail_search)

```
## Functions - 5/25
```{r Functions}
# functions
function_name <- function(x,y){
  print(x*y)

}

function_name(2,3)
```
## Groupby - 5/30
```{r, echo=TRUE, eval=FALSE}
head(mtcars)

# we want to know the average mpg for all clylinder types

# manual way 
mean(mtcars[mtcars$cyl==4, ]$mpg)
mean(mtcars[mtcars$cyl==6, ]$mpg)
mean(mtcars[mtcars$cyl==8, ]$mpg)

# groupby way
library(dplyr)
group_by(mtcars, cyl)
# lazy valuation = the only time a computer makes a calculation is when it is told to... so this group_by() function doesn't actually make R calculate anything.
summarize(group_by(mtcars, cyl), mpg_mean = mean(mpg)) # <------- group_by way
summarize(group_by(mtcars, cyl), mpg_mean = mean(mpg), mpg_sd = sd(mpg))

# piping way
mtcars %>% group_by(cyl) %>% summarize(mpg_mean = mean(mpg), mpg_sd = sd(mpg))

# ------------------------------------------------------------------------------------------------
# control flow
# if statements

if (TRUE) {
  print(3)
}
print(10)

if (FALSE) {
  print(3)
}
print(10)

# all of the cars with cylinder==4 returns 'numeric' for class, cylinder==6 returns '6', all others return 'NA'
my_function <- function(row_data){
  cylinder <- row_data['cyl']
  if (cylinder == 4){
    return(class(cylinder)) 
  }
  if (cylinder == 6){
    return(6)
  }
  return(NA)
}

apply(mtcars, MARGIN = 1, my_function) # 1 indicates row-by-row, 2 indicates column-by-column

# if, else...
another_function <- function(row_data){
  cylinder <- row_data['cyl']
  if (cylinder == 4){
    output <- 'buy me'
  }
  else {
    output <- 'sell me'
  }
  return(output)
}
apply(mtcars, MARGIN = 1, another_function)

# tidy data
library(tidyr)
# .... stopped paying attention

library(stringr)
load("~/sdal/projects/limbo/warTimes.rdata")
dash <- str_detect(warTimes, '-') # returns T/F
warTimes[dash] # returns strings containing a dash

the_times <- str_split(warTimes, "(ACAEA) | -", n = 2) # string "ACAEA" used to separate begin and end dates... separate columns by "ACAEA"
# "|" is an or condition (if it doesn't find first, it'll look for second...)
# n = 2 tells function that we only want it to split into 2 parts, so if it finds ACAEA and -, it shouldn't split it into 3 parts

# apply family
# lapply (always gives you a list)
# sapply (tries to give you same data format back)
extract_first <- function(x){
  return(x[1])
}

sapply(the_times, extract_first)

# OR

sapply(the_times, function(x) {x[1]})

start_dates <- sapply(the_times, function(x) {x[1]})

str_extract(start_dates, '[0-9][0-9][0-9][0-9]')
```
## American Community Survey - 5/31
```{r}

```

## Plotting - 6/1
```{r Plotting, warning=FALSE, message=FALSE}
library(ggplot2)

hist(diamonds$price)
ggplot(diamonds, aes(x=price)) +geom_density()
ggplot(data= diamonds, mapping = aes(x = price, fill = cut)) + geom_histogram()
# discrete vars, use bar graphs
ggplot(data = diamonds, aes(x=cut, fill = color)) + geom_bar()

ggplot(diamonds) +
    geom_point(aes(x=carat, y=price, color = color)) +
    facet_wrap(~cut)

g <- ggplot(diamonds, aes(x=carat, y=price, fill = color)) + geom_hex()
g + theme_minimal()

library(ggthemes)
g <- ggplot(diamonds, aes(x=carat, y = price)) + geom_point()
g + theme_economist()
```

## Data.Table - 6/2
```{r data.table}
library(data.table)
setwd("~/Documents/SDAL/R/R Practice/")

# download.file("http://download.geonames.org/export/zip/GB_full.csv.zip", "GB_full.csv.zip")
dt <- fread(unzip(zipfile = "~/Documents/SDAL/R/R Practice/GB_full.csv.zip"))
names(dt)

# subset columns
dt_subset_col <- dt[ , .(V2, V3, V4)] # instead of c(), do .()
head(dt_subset_col)
class(dt_subset_col)

df_subset <- as.data.frame(dt_subset_col)
rm("df_subset", "dt_subset_col")

# subset rows
dt_subset_row <- dt[V4 == "England", ]
head(dt_subset_row)
# sort datatable by column
dt[order(V4)] # ascending
dt[order(-V4, V3)] # descending V4, ascending V3

# adding new columns
# instead of df$new_col <- df$var1 + df$var2 ....
dt[, new_col := V10 + V11] # use := insead of <-
names(dt)

# re-code a variable
dt[V8 == "Aberdeen City", V8 := 'Abr City']
head(dt[, V8])

# deleting columns
dt[, c('V6', 'V7') := NULL] # back to using c()....

# using built-in functions
head(dt)
dt[, .(average = mean(V10, na.rm = TRUE)), by = V4] # store mean(V10) for each V4 in new column called 'average'

# melting
aqdt <- as.data.table(airquality)
aqdt_melt <- data.table::melt(data = aqdt, 
                 id.vars = c("Month", "Day"),
                 measure.vars = c("Ozone", "Solar.R", "Wind", "Temp"),
                 variable.name = 'measurement',
                 value.name = 'reading')
dim(aqdt)
dim(aqdt_melt)

# casting
data.table::dcast(data = aqdt_melt, Month + Day ~ measurement) # this 'undoes' the melt process above

```

#### Modeling
```{r Modeling}
devtools::install_github('bi-sdal/sdalr', force = TRUE)

data("anscombe")

summary(anscombe)

sapply(1:4, function(x){
    cor(anscombe[, x], anscombe[, x+4])})

summary(lm(y1 ~ x1, data = anscombe))

data('anscombosaurus', package = 'sdalr')

summary(anscombosaurus)
summary(lm(y~x, data = anscombosaurus))
plot(anscombosaurus$x, anscombosaurus$y)

library(ggplot2)
diamonds <- diamonds

quantile(diamonds$price, c(.10, .25, .50, .75))

economics <- economics

cor(economics$pce, economics$psavert)

econ_subset <- economics[, c('pce', 'psavert', 'uempmed', 'unemploy')]

library(GGally)

GGally::ggpairs(data = econ_subset)
m <- glm(price ~ carat, data=diamonds)
summary(m)

m <- glm(price ~ carat + table + as.factor(color), data=diamonds)
summary(m)

library(broom)
broom::tidy(m)$estimate
# broom::augment(m)
broom::glance(m)

plot(m)
# do not want to see a pattern in the residual plot
# QQ plot should show straight line (depending on distribution of QQ plot)

mpg_bin <- function(mpg) {
    if (mpg > 22) {
        return('good')
    } else {
        return('poor')
    }
}

mpg_bin_int <- function(mpg) {
    if (mpg > 22) {
        return(1)
    } else {
        return(0)
    }
}

mtcars$mpg_bin <- sapply(X = mtcars$mpg, FUN = mpg_bin)
mtcars$mpg_bin_int <- sapply(X = mtcars$mpg, FUN = mpg_bin_int)
head(mtcars)

m <- glm(formula = mpg_bin_int ~ hp + wt, data = mtcars, family = binomial(link = 'logit'))
summary(m)

results <- broom::tidy(m)
results$or <- exp(results$estimate)
results

library(survival)
head(heart) # about people on waiting list for Harvard heart transplant

cox_model <- coxph(Surv(heart$start, heart$stop, heart$event) ~ age + year + as.factor(surgery) + as.factor(transplant), data = heart)
summary(cox_model)

plot(survfit(cox_model)) # kaplan meyer curve (everyone is alive on day zero, as the number of days increase, the solid line shows you how many people are left in your population)

m1 <- glm(price ~ carat, data = diamonds)
m2 <- glm(price ~ carat + as.factor(cut), data = diamonds)
m3 <- glm(price ~ carat + as.factor(cut) + depth, data = diamonds)
m4 <- glm(price ~ carat + as.factor(cut) + depth + table, data = diamonds)

# ways of comparing models
anova(m1, m2, m3, m4)

# AIC and BIC put penalty on every new variable you add to a model
# looking for the smallest number to identify best model
AIC(m1, m2, m3, m4)
BIC(m1, m2, m3, m4)

# cross-validation (cv) prediction error for generalized linear models (glm)
library(boot) 
cv1 <- cv.glm(data = diamonds, glmfit = m1, K = 5) # K = number of partitions you want your data in (5 and 10 are popular)
cv2 <- cv.glm(data = diamonds, glmfit = m2, K = 5)
cv3 <- cv.glm(data = diamonds, glmfit = m3, K = 5)
cv4 <- cv.glm(data = diamonds, glmfit = m4, K = 5)

# print errors from all cross validations; pick the one with the smallest errors
cv1$delta
cv2$delta
cv3$delta
cv4$delta


```

#### Bayesian Statistics
```{r}
# commands to run a simple 1-d mcmc.

# number of mcmc scans
Niter <- 1000  

# logdens = log density
logdens1 <- function(x){
 # returns the log of the standard normal density 
 # the density is not normalized here
 return(-.5*x^2)
}

# build a vector to store the mcmc output
x <- rep(0,Niter)

# initialize x[1] with a starting value
x[1] <- 20.0

# compute the log of the posterior density for x[1]
logpost <- logdens1(x[1])

# width of metropolis proposal 
a <- 3 # use a uniform U[-a,a] metropolis proposal (give or take 3)

# carry out the Metropolis sampling
for(i in 2:Niter){
  can = x[i-1] + runif(1,-a,a) # generate candidate centered at old value, give or take a
  logpostcan <- logdens1(can) # generate the posterior density of that
  u <- runif(1) # generate uniform to tell us whether or not we accept this value
  if(log(u) < (logpostcan - logpost)){ # if the uniform is < ratio of new/old posterior, then accept (leave current value at the old value)
     # accept the proposal
    x[i] <- can
    logpost <- logpostcan # update the posterior density
  } else{
  x[i] <- x[i-1] # set the new value to the old value; leave logpost as is.
  }
}

# make a plot of the output
# use par to tell R to plot a single image, and set the margin area
par(mfrow=c(1,2),oma=c(0,0,0,0))#,matlabr=c(4,4,1,1))
plot(1:Niter,x,type='l')
hist(x[-(1:100)],main=paste('a =',a,sep=''),xlab='x',probability=T)
lines(seq(-3,3,length=200),dnorm(seq(-3,3,length=200)))

browser()
# now lets look at what happens if we change a and the starting value.
# it'll be easier to make the mcmc iterations a function

mcmc1d <- function(x0,Niter,a,LOGDENS){
 # run a metropolis chain using U(x-a,x+a) proposals
 # x0 is the starting value
 # Niter is the number of scans in this metropolis chain
 # LOGDENS is the log of the density from which we want draws.
 
 # the function returns a vector holding the output from the Metropolis sample
 x <- rep(0,Niter)

 # initialize x[1] with a starting value
 x[1] <- x0

 # compute the log of the posterior density for x[1]
 logpost <- LOGDENS(x[1])

 # carry out the Metropolis sampling
 for(i in 2:Niter){
   can = x[i-1] + runif(1,-a,a)
   logpostcan <- LOGDENS(can)
   u <- runif(1)
   if(log(u) < (logpostcan - logpost)){
      # accept the proposal
     x[i] <- can
     logpost <- logpostcan
   } else{
   x[i] <- x[i-1] # set the new value to the old value; leave logpost as is.
   }
 }
 return(x)
}

# first, let's look at the effect of changing the starting value
# we'll look at a=20,10,5,0
# this should motivate ideas about "burn-in":  Should we discard
# some of the initial piece of the chain so that our estimates
# are more accurate?

x1 <- mcmc1d(20,1000,3,logdens1)
x2 <- mcmc1d(10,1000,3,logdens1)
x3 <- mcmc1d(5,1000,3,logdens1)
x4 <- mcmc1d(0,1000,3,logdens1)

# look at some plots

par(mfrow=c(2,2),oma=c(0,0,0,0),mar=c(4,4,1,1))
plot(1:Niter,x1,ylim=c(-3,21),type='l')
plot(1:Niter,x2,ylim=c(-3,21),type='l')
plot(1:Niter,x3,ylim=c(-3,21),type='l')
plot(1:Niter,x4,ylim=c(-3,21),type='l')

browser()
# now, let's look at the effect of changing the proposal width a
# we'll look at a=20,10,3,0.1
x1 <- mcmc1d(20,1000,20,logdens1)
x2 <- mcmc1d(20,1000,3,logdens1)
x3 <- mcmc1d(20,1000,1,logdens1)
x4 <- mcmc1d(20,1000,.1,logdens1)

# look at some plots

 # different widths - no labels
par(mfrow=c(2,2),oma=c(0,0,0,0),mar=c(4,4,1,1))
plot(1:Niter,x1,ylim=c(-3,21),type='l',xlab='iteration')
plot(1:Niter,x2,ylim=c(-3,21),type='l',xlab='iteration')
plot(1:Niter,x3,ylim=c(-3,21),type='l',xlab='iteration')
plot(1:Niter,x4,ylim=c(-3,21),type='l',xlab='iteration')

browser()

 # show the labels
par(mfrow=c(2,2),oma=c(0,0,0,0),mar=c(4,4,1,1))
plot(1:Niter,x1,ylim=c(-3,21),type='l',xlab='iteration')
text(700,17,'a=20')
plot(1:Niter,x2,ylim=c(-3,21),type='l',xlab='iteration')
text(700,17,'a=3')
plot(1:Niter,x3,ylim=c(-3,21),type='l',xlab='iteration')
text(700,17,'a=1')
plot(1:Niter,x4,ylim=c(-3,21),type='l',xlab='iteration')
text(700,17,'a=.1')

# LESSON: a high (bad?) a is good to get you near the distribution quickly, but once you're close to it it doesn't perform as well as a lower a.

browser()

# now, let's look at the effect of changing the density
# we'll look at a mixture of normals
# a question we can look at is what width is best for
# sampling from this density via the metropolis algorithm
# 

logdens2 <- function(x){
 # returns a mixture of normals
 # .6*N(mu=-1,sd=.4) + .4*N(mu=.8,sd=.4)
 # the density is logged
 dens <- .6*dnorm(x,-1,.4)+.4*dnorm(x,.8,.4)
 return(log(dens))
}

logdens3 <- function(x){
 # returns a mixture of normals
 # .6*N(mu=-1,sd=.4) + .4*U(0,1)
 # the density is logged
 dens <- .6*dnorm(x,-1,.4)+.4*(x>0)*(x<1)
 return(log(dens))
}

logdens4 <- function(x){
 # returns a mixture of normals
 # .6*U[-1.2,-.2] + .4*dnorm(x,1,.3)
 # the density is logged
 dens <- .6*(x > -1.2)*(x < -.2) +.4*dnorm(x,1,.3)
 return(log(dens))
}

browser()
# a plot of the density
x <- seq(-3,3,length=200)
par(mfrow=c(1,1),oma=c(0,0,0,0),mar=c(4,4,1,1))
plot(x,exp(logdens2(x)),type='l')

browser()

x1 <- mcmc1d(2,1000,20,logdens2)
x2 <- mcmc1d(2,1000,3,logdens2)
x3 <- mcmc1d(2,1000,1,logdens2)
x4 <- mcmc1d(2,1000,.2,logdens2)

# look at some plots

par(mfrow=c(2,2),oma=c(0,0,0,0),mar=c(4,4,1,1))
plot(1:Niter,x1,ylim=c(-3,5),type='l',xlab='iteration')
text(700,4,'a=20')
plot(1:Niter,x2,ylim=c(-3,5),type='l',xlab='iteration')
text(700,4,'a=3')
plot(1:Niter,x3,ylim=c(-3,5),type='l',xlab='iteration')
text(700,4,'a=1')
plot(1:Niter,x4,ylim=c(-3,5),type='l',xlab='iteration')
text(700,4,'a=.2')

browser()

par(mfrow=c(2,2),oma=c(0,0,0,0),mar=c(4,4,1,1))
hist(x1[-(1:100)],probability=TRUE,ylim=c(0,.75),xlim=c(-3,3),xlab='x',
  main='a=20')
lines(x,exp(logdens2(x)))
hist(x2[-(1:100)],probability=TRUE,ylim=c(0,.75),xlim=c(-3,3),xlab='x',
  main='a=3')
lines(x,exp(logdens2(x)))
hist(x3[-(1:100)],probability=TRUE,ylim=c(0,.75),xlim=c(-3,3),xlab='x',
  main='a=1')
lines(x,exp(logdens2(x)))
hist(x4[-(1:100)],probability=TRUE,ylim=c(0,.75),xlim=c(-3,3),xlab='x',
  main='a=.2')
lines(x,exp(logdens2(x)))

browser()

#
# for the students, they can look at the mixture of normals problem.  What is
# their estimate for the P(x > 1.5)?  What is their choice for proposal width a?
# Did you discard any of the initial part of the chain?
#

```

